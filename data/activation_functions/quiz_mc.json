{
  "version": "1.0",
  "questions": [
    {"id": "af_mc_001", "type": "multiple_choice", "question": "Why do neural networks need activation functions?", "options": ["To speed up training", "To introduce non-linearity", "To reduce overfitting", "To normalize outputs"], "correct_index": 1, "explanation": "Without activation functions, neural networks would just be linear transformations. Non-linearity enables learning complex patterns.", "difficulty": 1, "tags": ["purpose", "non-linearity"]},
    {"id": "af_mc_002", "type": "multiple_choice", "question": "Which activation function is most commonly used in modern deep networks?", "options": ["Sigmoid", "Tanh", "ReLU", "Step function"], "correct_index": 2, "explanation": "ReLU (Rectified Linear Unit) is standard for hidden layers. Simple, efficient, avoids vanishing gradients.", "difficulty": 1, "tags": ["relu", "popular"]},
    {"id": "af_mc_003", "type": "multiple_choice", "question": "What is the range of the sigmoid function?", "options": ["(-infinity, +infinity)", "(-1, 1)", "(0, 1)", "[0, infinity)"], "correct_index": 2, "explanation": "Sigmoid σ(x) = 1/(1+e^-x) outputs values between 0 and 1. Useful for probabilities.", "difficulty": 1, "tags": ["sigmoid", "range"]},
    {"id": "af_mc_004", "type": "multiple_choice", "question": "What problem does ReLU help solve compared to sigmoid?", "options": ["Overfitting", "Vanishing gradient problem", "Exploding gradients", "Slow inference"], "correct_index": 1, "explanation": "ReLU gradient is 1 (for x>0) or 0, avoiding vanishing gradients that plague sigmoid/tanh in deep networks.", "difficulty": 2, "tags": ["relu", "vanishing_gradient"]},
    {"id": "af_mc_005", "type": "multiple_choice", "question": "What is the formula for ReLU?", "options": ["max(0, x)", "1/(1+e^-x)", "tanh(x)", "x^2"], "correct_index": 0, "explanation": "ReLU(x) = max(0, x). Outputs x if positive, 0 if negative. Simple and effective.", "difficulty": 1, "tags": ["relu", "formula"]},
    {"id": "af_mc_006", "type": "multiple_choice", "question": "Which activation is typically used in the output layer for binary classification?", "options": ["ReLU", "Sigmoid", "Tanh", "Softmax"], "correct_index": 1, "explanation": "Sigmoid outputs probability [0,1] for binary classification. Softmax for multi-class.", "difficulty": 1, "tags": ["sigmoid", "classification"]},
    {"id": "af_mc_007", "type": "multiple_choice", "question": "What is the 'dying ReLU' problem?", "options": ["ReLU becomes too slow", "Neurons get stuck outputting 0", "ReLU causes overfitting", "ReLU gradients explode"], "correct_index": 1, "explanation": "If ReLU neuron outputs 0, gradient is 0, weights never update. Neuron 'dies'. Leaky ReLU fixes this.", "difficulty": 2, "tags": ["relu", "problems"]},
    {"id": "af_mc_008", "type": "multiple_choice", "question": "What is the range of tanh?", "options": ["(0, 1)", "(-1, 1)", "(-infinity, +infinity)", "[0, infinity)"], "correct_index": 1, "explanation": "tanh outputs between -1 and 1. Zero-centered, unlike sigmoid. Similar to scaled sigmoid.", "difficulty": 1, "tags": ["tanh", "range"]},
    {"id": "af_mc_009", "type": "multiple_choice", "question": "Leaky ReLU differs from ReLU by:", "options": ["Being faster", "Having small negative slope for x<0", "Working only for positive values", "Normalizing outputs"], "correct_index": 1, "explanation": "Leaky ReLU(x) = x if x>0, else αx (α≈0.01). Prevents dying ReLU by allowing small negative gradients.", "difficulty": 2, "tags": ["leaky_relu", "variants"]},
    {"id": "af_mc_010", "type": "multiple_choice", "question": "For multi-class classification output, we use:", "options": ["ReLU", "Sigmoid", "Softmax", "Tanh"], "correct_index": 2, "explanation": "Softmax converts logits to probability distribution over K classes. Outputs sum to 1.", "difficulty": 1, "tags": ["softmax", "multiclass"]}
  ]
}
