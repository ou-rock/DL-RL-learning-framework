{
  "version": "1.0",
  "questions": [
    {"id": "ql_mc_001", "type": "multiple_choice", "question": "What does Q stand for in Q-learning?", "options": ["Quantum", "Quality (value)", "Quick", "Quotient"], "correct_index": 1, "explanation": "Q represents Quality or value of taking action a in state s. Q(s,a) = expected cumulative reward.", "difficulty": 1, "tags": ["terminology", "q_function"]},
    {"id": "ql_mc_002", "type": "multiple_choice", "question": "Q-learning is a ___ reinforcement learning algorithm.", "options": ["Model-based", "Model-free", "Policy-based", "Actor-critic"], "correct_index": 1, "explanation": "Q-learning is model-free (doesn't learn environment model) and value-based (learns Q-function, derives policy).", "difficulty": 1, "tags": ["classification", "model_free"]},
    {"id": "ql_mc_003", "type": "multiple_choice", "question": "What is the Q-learning update rule?", "options": ["Q(s,a) = r", "Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]", "Q(s,a) = max(r)", "Q(s,a) ← r + γQ(s',a')"], "correct_index": 1, "explanation": "Bellman update: Q ← Q + α[target - Q] where target = r + γ max_a' Q(s',a'). TD learning.", "difficulty": 2, "tags": ["update_rule", "bellman"]},
    {"id": "ql_mc_004", "type": "multiple_choice", "question": "What does γ (gamma) represent in Q-learning?", "options": ["Learning rate", "Discount factor", "Exploration rate", "Temperature"], "correct_index": 1, "explanation": "γ ∈ [0,1] is discount factor. γ=0: myopic (immediate reward), γ→1: far-sighted (future reward).", "difficulty": 1, "tags": ["discount_factor", "parameters"]},
    {"id": "ql_mc_005", "type": "multiple_choice", "question": "How does Q-learning select actions during training?", "options": ["Always greedy (max Q)", "ε-greedy (explore vs exploit)", "Random only", "Policy gradient"], "correct_index": 1, "explanation": "ε-greedy: with probability ε explore (random action), else exploit (argmax Q). Balances exploration/exploitation.", "difficulty": 2, "tags": ["exploration", "epsilon_greedy"]},
    {"id": "ql_mc_006", "type": "multiple_choice", "question": "What is the goal of Q-learning?", "options": ["Learn environment model", "Learn optimal Q-function Q*", "Learn state distribution", "Minimize loss"], "correct_index": 1, "explanation": "Goal: learn optimal Q*(s,a) that gives maximum expected return. Optimal policy: π*(s) = argmax_a Q*(s,a).", "difficulty": 1, "tags": ["goal", "optimal_policy"]},
    {"id": "ql_mc_007", "type": "multiple_choice", "question": "Q-learning is ___ policy.", "options": ["On-policy", "Off-policy", "Both", "Neither"], "correct_index": 1, "explanation": "Off-policy: learns about greedy policy (max Q) while following exploratory policy (ε-greedy). SARSA is on-policy.", "difficulty": 2, "tags": ["off_policy", "classification"]},
    {"id": "ql_mc_008", "type": "multiple_choice", "question": "In tabular Q-learning, how is Q(s,a) stored?", "options": ["Neural network", "Table/matrix", "Decision tree", "Hash function"], "correct_index": 1, "explanation": "Tabular Q-learning stores Q-values in table (rows=states, cols=actions). For large state spaces, use function approximation (DQN).", "difficulty": 1, "tags": ["tabular", "representation"]},
    {"id": "ql_mc_009", "type": "multiple_choice", "question": "What does α (alpha) control in Q-learning?", "options": ["Discount factor", "Learning rate", "Exploration rate", "Reward scale"], "correct_index": 1, "explanation": "α ∈ (0,1] is learning rate. High α: fast learning, high variance. Low α: slow learning, low variance.", "difficulty": 1, "tags": ["learning_rate", "parameters"]},
    {"id": "ql_mc_010", "type": "multiple_choice", "question": "Deep Q-Network (DQN) extends Q-learning by:", "options": ["Using policy gradient", "Approximating Q with neural network", "Being model-based", "Removing discount factor"], "correct_index": 1, "explanation": "DQN uses neural network to approximate Q(s,a) for large state spaces. Adds experience replay and target network.", "difficulty": 2, "tags": ["dqn", "deep_rl"]}
  ]
}
